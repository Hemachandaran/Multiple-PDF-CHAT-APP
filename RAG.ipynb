{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse PDF Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader('RAG/DATA',glob=\"./*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents= loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2024-04-10T21:11:43+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2024-04-10T21:11:43+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf',\n",
       " 'total_pages': 15,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.21',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2025-01-12T12:07:39-08:00',\n",
       " 'author': '',\n",
       " 'title': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2025-01-12T12:07:39-08:00',\n",
       " 'trapped': '/False',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       " 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf',\n",
       " 'total_pages': 20,\n",
       " 'page': 1,\n",
       " 'page_label': '2'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[16].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2505"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = Pdk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead(Q, K, V) = Concat(head1, ...,headh)WO\n",
      "where headi = Attention(QWQ\n",
      "i , KWK\n",
      "i , V WV\n",
      "i )\n",
      "Where the projections are parameter matricesWQ\n",
      "i ∈ Rdmodel×dk , WK\n",
      "i ∈ Rdmodel×dk , WV\n",
      "i ∈ Rdmodel×dv\n",
      "and WO ∈ Rhdv×dmodel .\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\n",
      "5\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
      "size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2 · d) O(1) O(1)\n",
      "Recurrent O(n · d2) O(n) O(n)\n",
      "Convolutional O(k · n · d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r · n · d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "P E(pos,2i) = sin(pos/100002i/dmodel )\n",
      "P E(pos,2i+1) = cos(pos/100002i/dmodel )\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\n",
      "P Epos.\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "6\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      "or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
      "considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate = d−0.5\n",
      "model · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup_steps = 4000.\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "7\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "Model\n",
      "BLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0 · 1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\n",
      "ConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\n",
      "MoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\n",
      "Deep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\n",
      "GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\n",
      "ConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\n",
      "Transformer (base model) 27.3 38.1 3.3 · 1018\n",
      "Transformer (big) 28.4 41.8 2.3 · 1019\n",
      "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop = 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU 5.\n",
      "6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "N d model dff h d k dv Pdrop ϵls\n",
      "train PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)\n",
      "1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B) 16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)\n",
      "2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)\n",
      "0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      "results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, discriminative 91.3\n",
      "Zhu et al. (2013) [40] semi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi-supervised 92.1\n",
      "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
      "Transformer (4 layers) semi-supervised 92.7\n",
      "Luong et al. (2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\n",
      "for both WSJ only and the semi-supervised setting.\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Recurrent Neural Network Grammar [8].\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\n",
      "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "References\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450, 2016.\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "10\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR, abs/1406.1078, 2014.\n",
      "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357, 2016.\n",
      "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "network grammars. In Proc. of NAACL, 2016.\n",
      "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      "arXiv:1308.0850, 2013.\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pages 770–778, 2016.\n",
      "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
      "9(8):1735–1780, 1997.\n",
      "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Language Processing, pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR), 2016.\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "11\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
      "2006.\n",
      "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "preprint arXiv:1608.05859, 2016.\n",
      "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research, 15(1):1929–1958, 2014.\n",
      "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems, 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers), pages 434–443. ACL, August 2013.\n",
      "12\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\n",
      "rights reserved. Draft of January 12, 2025.\n",
      "CHAPTER\n",
      "9\n",
      "The Transformer\n",
      "“The true art of memory is the art of attention ”\n",
      "Samuel Johnson, Idler #74, September 1759\n",
      "In this chapter we introduce thetransformer, the standard architecture for build-\n",
      "ing large language models. Transformer-based large language models have com-\n",
      "pletely changed the ﬁeld of speech and language processing. Indeed, every subse-\n",
      "quent chapter in this textbook will make use of them. We’ll focus for now on left-\n",
      "to-right (sometimes called causal or autoregressive) language modeling, in which\n",
      "we are given a sequence of input tokens and predict output tokens one by one by\n",
      "conditioning on the prior context.\n",
      "The transformer is a neural network with a speciﬁc structure that includes a\n",
      "mechanism called self-attention or multi-head attention.1 Attention can be thought\n",
      "of as a way to build contextual representations of a token’s meaning byattending to\n",
      "and integrating information from surrounding tokens, helping the model learn how\n",
      "tokens relate to each other over large spans.\n",
      "Stacked\n",
      "Transformer\n",
      "Blocks\n",
      "So long and thanks for\n",
      "long and thanks forNext token all\n",
      "…\n",
      "…\n",
      "…\n",
      "U\n",
      "Input tokens\n",
      "x1 x2\n",
      "Language\n",
      "Modeling\n",
      "Head\n",
      "x3 x4 x5\n",
      "Input\n",
      "Encoding\n",
      " E\n",
      "1+\n",
      "E\n",
      "2+\n",
      "E\n",
      "3+\n",
      "E\n",
      "4+\n",
      "E\n",
      "5+\n",
      "…\n",
      "… ………\n",
      "U\n",
      " U\n",
      " U\n",
      " U\n",
      "…\n",
      "logits logits logits logits logits\n",
      "Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\n",
      "get encoded, passed through a set of stacked transformer blocks, and then a language model\n",
      "head that predicts the next token.\n",
      "Fig. 9.1 sketches the transformer architecture. A transformer has three major\n",
      "components. At the center are columns of transformer blocks. Each block is a\n",
      "multilayer network (a multi-head attention layer, feedforward networks and layer\n",
      "normalization steps) that maps an input vectorxi in column i (corresponding to input\n",
      "1 Although multi-head attention developed historically from theRNN attention mechanism (Chapter 8),\n",
      "we’ll deﬁne attention from scratch here for readers who haven’t yet read Chapter 8.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 2 CHAPTER 9 • T HE TRANSFORMER\n",
      "token i) to an output vector hi. The set of n blocks maps an entire context window\n",
      "of input vectors (x1,..., xn) to a window of output vectors (h1,..., hn) of the same\n",
      "length. A column might contain from 12 to 96 or more stacked blocks.\n",
      "The column of blocks is preceded by theinput encoding component, which pro-\n",
      "cesses an input token (like the wordthanks) into a contextual vector representation,\n",
      "using an embedding matrix E and a mechanism for encoding token position. Each\n",
      "column is followed by a language modeling head, which takes the embedding out-\n",
      "put by the ﬁnal transformer block, passes it through anunembedding matrix U and\n",
      "a softmax over the vocabulary to generate a single token for that column.\n",
      "Transformer-based language models are complex, and so the details will unfold\n",
      "over the next 5 chapters. In the next sections we’ll introduce multi-head attention,\n",
      "the rest of the transformer block, and the input encoding and language modeling\n",
      "head components. Chapter 10 discusses how language models are pretrained, and\n",
      "how tokens are generated via sampling. Chapter 11 introduces masked language\n",
      "modeling and the BERT family of bidirectional transformer encoder models. Chap-\n",
      "ter 12 shows how toprompt LLMs to perform NLP tasks by giving instructions and\n",
      "demonstrations, and how to align the model with human preferences. Chapter 13\n",
      "will introduce machine translation with the encoder-decoder architecture.\n",
      "9.1 Attention\n",
      "Recall from Chapter 6 that for word2vec and other static embeddings, the repre-\n",
      "sentation of a word’s meaning is always the same vector irrespective of the context:\n",
      "the word chicken, for example, is always represented by the same ﬁxed vector. So\n",
      "a static vector for the word it might somehow encode that this is a pronoun used\n",
      "for animals and inanimate entities. But in context it has a much richer meaning.\n",
      "Consider it in one of these two sentences:\n",
      "(9.1) The chicken didn’t cross the road becauseit was too tired.\n",
      "(9.2) The chicken didn’t cross the road becauseit was too wide.\n",
      "In (9.1) it is the chicken (i.e., the reader knows that the chicken was tired), while\n",
      "in (9.2) it is the road (and the reader knows that the road was wide). 2 That is, if\n",
      "we are to compute the meaning of this sentence, we’ll need the meaning ofit to be\n",
      "associated with the chickenin the ﬁrst sentence and associated with the roadin\n",
      "the second one, sensitive to the context.\n",
      "Furthermore, consider reading left to right like a causal language model, pro-\n",
      "cessing the sentence up to the word it:\n",
      "(9.3) The chicken didn’t cross the road becauseit\n",
      "At this point we don’t yet know which thingit is going to end up referring to! So a\n",
      "representation of it at this point might have aspects of both chicken and road as\n",
      "the reader is trying to guess what happens next.\n",
      "This fact that words have rich linguistic relationships with other words that may\n",
      "be far away pervades language. Consider two more examples:\n",
      "(9.4) The keys to the cabinet are on the table.\n",
      "(9.5) I walked along the pond, and noticed one of the trees along the bank.\n",
      "2 We say that in the ﬁrst example it corefers with the chicken, and in the second it corefers with the\n",
      "road; we’ll return to this in Chapter 23.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.1 • A TTENTION 3\n",
      "In (9.4), the phrase The keys is the subject of the sentence, and in English and many\n",
      "languages, must agree in grammatical number with the verbare; in this case both are\n",
      "plural. In English we can’t use a singular verb like is with a plural subject like keys\n",
      "(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\n",
      "to the side of a pond or river and not a ﬁnancial institution because of the context,\n",
      "including words like pond. (We’ll discuss word senses more in Chapter 11.)\n",
      "The point of all these examples is that these contextual words that help us com-\n",
      "pute the meaning of words in context can be quite far away in the sentence or para-\n",
      "graph. Transformers can build contextual representations of word meaning, contex-\n",
      "tual embeddings, by integrating the meaning of these helpful contextual words. In acontextual\n",
      "embeddings\n",
      "transformer, layer by layer, we build up richer and richer contextualized representa-\n",
      "tions of the meanings of input tokens. At each layer, we compute the representation\n",
      "of a token i by combining information about i from the previous layer with infor-\n",
      "mation about the neighboring tokens to produce a contextualized representation for\n",
      "each word at each position.\n",
      "Attention is the mechanism in the transformer that weighs and combines the\n",
      "representations from appropriate other tokens in the context from layerk−1 to build\n",
      "the representation for tokens in layer k.\n",
      "The\n",
      "chicken\n",
      "didn’t\n",
      "cross\n",
      "the\n",
      "road\n",
      "because\n",
      "it\n",
      "was\n",
      "too\n",
      "tired\n",
      "The\n",
      "chicken\n",
      "didn’t\n",
      "cross\n",
      "the\n",
      "road\n",
      "because\n",
      "it\n",
      "was\n",
      "too\n",
      "tired\n",
      "Layer k+1\n",
      "Layer k\n",
      "self-attention distribution\n",
      "columns corresponding to input tokens\n",
      "Figure 9.2 The self-attention weight distribution α that is part of the computation of the\n",
      "representation for the word it at layer k +1. In computing the representation for it, we attend\n",
      "differently to the various words at layer k, with darker shades indicating higher self-attention\n",
      "values. Note that the transformer is attending highly to the columns corresponding to the\n",
      "tokens chicken and road , a sensible result, since at the point whereit occurs, it could plausibly\n",
      "corefer with the chicken or the road, and hence we’d like the representation for it to draw on\n",
      "the representation for these earlier words. Figure adapted from Uszkoreit (2017).\n",
      "Fig. 9.2 shows a schematic example simpliﬁed from a transformer (Uszkoreit,\n",
      "2017). The ﬁgure describes the situation when the current token is it and we need\n",
      "to compute a contextual representation for this token at layerk+1 of the transformer,\n",
      "drawing on the representations (from layer k) of every prior token. The ﬁgure uses\n",
      "color to represent the attention distribution over the contextual words: the tokens\n",
      "chicken and road both have a high attention weight, meaning that as we are com-\n",
      "puting the representation for it, we will draw most heavily on the representation for\n",
      "chicken and road. This will be useful in building the ﬁnal representation for it,\n",
      "since it will end up coreferring with either chicken or road.\n",
      "Let’s now turn to how this attention distribution is represented and computed.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 2, 'page_label': '3'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 4 CHAPTER 9 • T HE TRANSFORMER\n",
      "9.1.1 Attention more formally\n",
      "As we’ve said, the attention computation is a way to compute a vector representation\n",
      "for a token at a particular layer of a transformer, by selectively attending to and\n",
      "integrating information from prior tokens at the previous layer. Attention takes an\n",
      "input representation xi corresponding to the input token at position i, and a context\n",
      "window of prior inputs x1..xi−1, and produces an output ai.\n",
      "In causal, left-to-right language models, the context is any of the prior words.\n",
      "That is, when processing xi, the model has access toxi as well as the representations\n",
      "of all the prior tokens in the context window (context windows consist of thousands\n",
      "of tokens) but no tokens afteri. (By contrast, in Chapter 11 we’ll generalize attention\n",
      "so it can also look ahead to future words.)\n",
      "Fig. 9.3 illustrates this ﬂow of information in an entire causal self-attention layer,\n",
      "in which this same attention computation happens in parallel at each token position\n",
      "i. Thus a self-attention layer maps input sequences (x1,..., xn) to output sequences\n",
      "of the same length (a1,..., an).\n",
      "attentionattentionSelf-Attention\n",
      "Layer\n",
      "attentionattentionattention\n",
      "a1 a2 a3 a4 a5\n",
      "x3 x4 x5x1 x2\n",
      "Figure 9.3 Information ﬂow in causal self-attention. When processing each input xi, the\n",
      "model attends to all the inputs up to, and including xi.\n",
      "Simpliﬁed version of attention At its heart, attention is really just a weighted\n",
      "sum of context vectors, with a lot of complications added to how the weights are\n",
      "computed and what gets summed. For pedagogical purposes let’s ﬁrst describe a\n",
      "simpliﬁed intuition of attention, in which the attention output ai at token position i\n",
      "is simply the weighted sum of all the representations xj, for all j ≤i; we’ll use αi j\n",
      "to mean how much xj should contribute to ai:\n",
      "Simpliﬁed version: ai =\n",
      "∑\n",
      "j≤i\n",
      "αi jxj (9.6)\n",
      "Each αi j is a scalar used for weighing the value of input xj when summing up\n",
      "the inputs to compute ai. How shall we compute this α weighting? In attention we\n",
      "weight each prior embedding proportionally to howsimilar it is to the current token\n",
      "i. So the output of attention is a sum of the embeddings of prior tokens weighted\n",
      "by their similarity with the current token embedding. We compute similarity scores\n",
      "via dot product, which maps two vectors into a scalar value ranging from −∞ to\n",
      "∞. The larger the score, the more similar the vectors that are being compared. We’ll\n",
      "normalize these scores with a softmax to create the vector of weights αi j, j ≤i.\n",
      "Simpliﬁed Version: score(xi,xj) = xi ·xj (9.7)\n",
      "αi j = softmax(score(xi,xj)) ∀j ≤i (9.8)\n",
      "Thus in Fig. 9.3 we computea3 by computing three scores: x3 ·x1, x3 ·x2 and x3 ·x3,\n",
      "normalizing them by a softmax, and using the resulting probabilities as weights\n",
      "indicating each of their proportional relevance to the current position i. Of course,\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.1 • A TTENTION 5\n",
      "the softmax weight will likely be highest for xi, since xi is very similar to itself,\n",
      "resulting in a high dot product. But other context words may also be similar toi, and\n",
      "the softmax will also assign some weight to those words. Then we use these weights\n",
      "as the α values in Eq. 9.6 to compute the weighted sum that is our a3.\n",
      "The simpliﬁed attention in equations 9.6 – 9.8 demonstrates the attention-based\n",
      "approach to computing ai: compare the xi to prior vectors, normalize those scores\n",
      "into a probability distribution used to weight the sum of the prior vector. But now\n",
      "we’re ready to remove the simpliﬁcations.\n",
      "A single attention head using query, key, and value matrices Now that we’ve\n",
      "seen a simple intuition of attention, let’s introduce the actual attention head, theattention head\n",
      "version of attention that’s used in transformers. (The word head is often used inhead\n",
      "transformers to refer to speciﬁc structured layers). The attention head allows us to\n",
      "distinctly represent three different roles that each input embedding plays during the\n",
      "course of the attention process:\n",
      "• As the current element being compared to the preceding inputs. We’ll refer to\n",
      "this role as a query.query\n",
      "• In its role as a preceding input that is being compared to the current element\n",
      "to determine a similarity weight. We’ll refer to this role as akey.key\n",
      "• And ﬁnally, as a value of a preceding element that gets weighted and summedvalue\n",
      "up to compute the output for the current element.\n",
      "To capture these three different roles, transformers introduce weight matrices\n",
      "WQ, WK, and WV. These weights will project each input vector xi into a represen-\n",
      "tation of its role as a key, query, or value:\n",
      "qi = xiWQ; ki = xiWK; vi = xiWV (9.9)\n",
      "Given these projections, when we are computing the similarity of the current ele-\n",
      "ment xi with some prior element xj, we’ll use the dot product between the current\n",
      "element’squery vector qi and the preceding element’skey vector kj. Furthermore,\n",
      "the result of a dot product can be an arbitrarily large (positive or negative) value, and\n",
      "exponentiating large values can lead to numerical issues and loss of gradients during\n",
      "training. To avoid this, we scale the dot product by a factor related to the size of the\n",
      "embeddings, via dividing by the square root of the dimensionality of the query and\n",
      "key vectors (dk). We thus replace the simpliﬁed Eq. 9.7 with Eq. 9.11. The ensuing\n",
      "softmax calculation resulting in αi j remains the same, but the output calculation for\n",
      "headi is now based on a weighted sum over the value vectors v (Eq. 9.13).\n",
      "Here’s a ﬁnal set of equations for computing self-attention for a single self-\n",
      "attention output vector ai from a single input vector xi. This version of attention\n",
      "computes ai by summing the values of the prior elements, each weighted by the\n",
      "similarity of its key to the query from the current element:\n",
      "qi = xiWQ; kj = xjWK; vj = xjWV (9.10)\n",
      "score(xi,xj) = qi ·kj√dk\n",
      "(9.11)\n",
      "αi j = softmax(score(xi,xj)) ∀j ≤i (9.12)\n",
      "headi =\n",
      "∑\n",
      "j≤i\n",
      "αi jvj (9.13)\n",
      "ai = headiWO (9.14)\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 4, 'page_label': '5'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 6 CHAPTER 9 • T HE TRANSFORMER\n",
      "6. Sum the weighted \n",
      "value vectors\n",
      "4. Turn into 𝛼i,j weights via softmax\n",
      "a3\n",
      "1. Generate \n",
      "key, query, value \n",
      "vectors\n",
      "2. Compare x3’s query with\n",
      "the keys for x1, x2, and x3\n",
      "8. Output of self-attention\n",
      "×\n",
      "×\n",
      "x1\n",
      "k q v\n",
      "WK WQ WV\n",
      "5. Weigh each value vector\n",
      "÷\n",
      "√dk\n",
      "3. Divide scalar score by √dk\n",
      "÷\n",
      "√dk\n",
      "÷\n",
      "√dk\n",
      "𝛼3,1 𝛼3,2 𝛼3,3\n",
      "x2\n",
      "k q v\n",
      "WK WQ WV\n",
      "x3\n",
      "k q v\n",
      "WK WQ WV\n",
      "WO\n",
      "[1 × d]\n",
      "[1 × d]\n",
      "[dv × d]\n",
      "[1 × dv]\n",
      "[1 × dv][1 × dv][1 × dv]\n",
      "[1 × dv] [1 × dv] [1 x dv]\n",
      "7. Reshape to [1 x d] \n",
      "[1 × d] [1 × d]\n",
      "Figure 9.4 Calculating the value of a3, the third element of a sequence using causal (left-\n",
      "to-right) self-attention.\n",
      "We illustrate this in Fig. 9.4 for the case of calculating the value of the third output\n",
      "a3 in a sequence.\n",
      "Note that we’ve also introduced one more matrix,WO, which is right-multiplied\n",
      "by the attention head. This is necessary to reshape the output of the head. The input\n",
      "to attention xi and the output from attention ai both have the same dimensionality\n",
      "[1 ×d]. We often call d the model dimensionality, and indeed as we’ll discuss in\n",
      "Section 9.2 the output hi of each transformer block, as well as the intermediate vec-\n",
      "tors inside the transformer block also have the same dimensionality [1 ×d]. Having\n",
      "everything be the same dimensionality makes the transformer very modular.\n",
      "So let’s talk shapes. How do we get from [1 ×d] at the input to [1 ×d] at the\n",
      "output? Let’s look at all the internal shapes. We’ll have a dimension dk for the key\n",
      "and query vectors. The query vector and the key vector are both dimensionality\n",
      "1 ×dk, so we can take their dot product qi ·kj to produce a scalar. We’ll have a\n",
      "separate dimension dv for the value vectors. The transform matrix WQ has shape\n",
      "[d ×dk], WK is [d ×dk], and WV is [d ×dv]. So the output of headi in equation\n",
      "Eq. 9.13 is of shape [1 ×dv]. To get the desired output shape [1 ×d] we’ll need to\n",
      "reshape the head output, and so WO is of shape [dv ×d]. In the original transformer\n",
      "work (Vaswani et al., 2017),d was 512, dk and dv were both 64.\n",
      "Multi-head Attention Equations 9.11-9.13 describe a single attention head. But\n",
      "actually, transformers use multiple attention heads. The intuition is that each head\n",
      "might be attending to the context for different purposes: heads might be special-\n",
      "ized to represent different linguistic relationships between context elements and the\n",
      "current token, or to look for particular kinds of patterns in the context.\n",
      "So in multi-head attention we have A separate attention heads that reside inmulti-head\n",
      "attention\n",
      "parallel layers at the same depth in a model, each with its own set of parameters that\n",
      "allows the head to model different aspects of the relationships among inputs. Thus\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 5, 'page_label': '6'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.1 • A TTENTION 7\n",
      "each head i in a self-attention layer has its own set of key, query and value matrices:\n",
      "WKi, WQi and WVi. These are used to project the inputs into separate key, value,\n",
      "and query embeddings for each head.\n",
      "When using multiple heads the model dimension d is still used for the input\n",
      "and output, the key and query embeddings have dimensionality dk, and the value\n",
      "embeddings are of dimensionality dv (again, in the original transformer paper dk =\n",
      "dv = 64, A = 8, and d = 512). Thus for each head i, we have weight layers WQi of\n",
      "shape [d ×dk], WKi of shape [d ×dk], and WVi of shape [d ×dv].\n",
      "Below are the equations for attention augmented with multiple heads; Fig. 9.5\n",
      "shows an intuition.\n",
      "qc\n",
      "i = xiWQc; kc\n",
      "j = xjWKc; vc\n",
      "j = xjWVc; ∀c 1 ≤c ≤A (9.15)\n",
      "scorec(xi,xj) =\n",
      "qc\n",
      "i ·kc\n",
      "j√dk\n",
      "(9.16)\n",
      "αc\n",
      "i j = softmax(scorec(xi,xj)) ∀j ≤i (9.17)\n",
      "headc\n",
      "i =\n",
      "∑\n",
      "j≤i\n",
      "αc\n",
      "i jvc\n",
      "j (9.18)\n",
      "ai = (head1 ⊕head2...⊕headA)WO (9.19)\n",
      "MultiHeadAttention(xi,[x1,···,xN ]) = ai (9.20)\n",
      "The output of each of the A heads is of shape 1 ×dv, and so the output of the\n",
      "multi-head layer with A heads consists of A vectors of shape 1 ×dv. These are\n",
      "concatenated to produce a single output with dimensionality 1 ×hdv. Then we use\n",
      "yet another linear projection WO ∈RAdv×d to reshape it, resulting in the multi-head\n",
      "attention vector ai with the correct output shape [1 ×d] at each input i.\n",
      "ai\n",
      "xi-1 xixi-2xi-3\n",
      "WK\n",
      "1\n",
      "Head 1\n",
      "WV\n",
      "1 WQ\n",
      "1\n",
      "…\n",
      "…\n",
      "WK\n",
      "2\n",
      "Head 2\n",
      "WV\n",
      "2 WQ\n",
      "2 WK\n",
      "8\n",
      "Head 8\n",
      "WV\n",
      "8 WQ\n",
      "8\n",
      "ai\n",
      "WO  [hdv x d]\n",
      "[1 x dv ]\n",
      "[1 x d]\n",
      "[1 x d]\n",
      "[1 x hdv ]\n",
      "Project down to d\n",
      "Concatenate Outputs\n",
      "Each head\n",
      "attends diﬀerently\n",
      "to context\n",
      "…\n",
      "[1 x dv ]\n",
      "Figure 9.5 The multi-head attention computation for input xi, producing output ai. A multi-head attention\n",
      "layer has A heads, each with its own key, query and value weight matrices. The outputs from each of the heads\n",
      "are concatenated and then projected down to d, thus producing an output of the same size as the input.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 6, 'page_label': '7'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 8 CHAPTER 9 • T HE TRANSFORMER\n",
      "9.2 Transformer Blocks\n",
      "The self-attention calculation lies at the core of what’s called a transformer block,\n",
      "which, in addition to the self-attention layer, includes three other kinds of layers: (1)\n",
      "a feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-\n",
      "ally called “layer norm”).\n",
      "Layer Norm\n",
      "xi\n",
      "+\n",
      "hi-1\n",
      "Layer Norm\n",
      "MultiHead\n",
      "Attention\n",
      "Feedforward\n",
      "xi-1 xi+1\n",
      "hi hi+1\n",
      "+\n",
      "……\n",
      "Residual\n",
      "Stream\n",
      "Figure 9.6 The architecture of a transformer block showing the residual stream. This\n",
      "ﬁgure shows the prenorm version of the architecture, in which the layer norms happen before\n",
      "the attention and feedforward layers rather than after.\n",
      "Fig. 9.6 illustrates a transformer block, sketching a common way of thinking\n",
      "about the block that is called the residual stream (Elhage et al., 2021). In the resid-residual stream\n",
      "ual stream viewpoint, we consider the processing of an individual token i through\n",
      "the transformer block as a single stream of d-dimensional representations for token\n",
      "position i. This residual stream starts with the original input vector, and the various\n",
      "components read their input from the residual stream and add their output back into\n",
      "the stream.\n",
      "The input at the bottom of the stream is an embedding for a token, which has\n",
      "dimensionality d. This initial embedding gets passed up (by residual connections),\n",
      "and is progressively added to by the other components of the transformer: the at-\n",
      "tention layer that we have seen, and the feedforward layer that we will introduce.\n",
      "Before the attention and feedforward layer is a computation called the layer norm.\n",
      "Thus the initial vector is passed through a layer norm and attention layer, and\n",
      "the result is added back into the stream, in this case to the original input vector\n",
      "xi. And then this summed vector is again passed through another layer norm and a\n",
      "feedforward layer, and the output of those is added back into the residual, and we’ll\n",
      "use hi to refer to the resulting output of the transformer block for token i. (In earlier\n",
      "descriptions the residual stream was often described using a different metaphor as\n",
      "residual connections that add the input of a component to its output, but the residual\n",
      "stream is a more perspicuous way of visualizing the transformer.)\n",
      "We’ve already seen the attention layer, so let’s now introduce the feedforward\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 7, 'page_label': '8'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.2 • T RANSFORMER BLOCKS 9\n",
      "and layer norm computations in the context of processing a single input xi at token\n",
      "position i.\n",
      "Feedforward layer The feedforward layer is a fully-connected 2-layer network,\n",
      "i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights\n",
      "are the same for each token position i , but are different from layer to layer. It\n",
      "is common to make the dimensionality dff of the hidden layer of the feedforward\n",
      "network be larger than the model dimensionality d. (For example in the original\n",
      "transformer model, d = 512 and dff = 2048.)\n",
      "FFN(xi) =ReLU(xiW1 +b1)W2 +b2 (9.21)\n",
      "Layer Norm At two stages in the transformer block we normalize the vector (Ba\n",
      "et al., 2016). This process, called layer norm (short for layer normalization), is onelayer norm\n",
      "of many forms of normalization that can be used to improve training performance\n",
      "in deep neural networks by keeping the values of a hidden layer in a range that\n",
      "facilitates gradient-based training.\n",
      "Layer norm is a variation of the z-score from statistics, applied to a single vec-\n",
      "tor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm\n",
      "is not applied to an entire transformer layer, but just to the embedding vector of a\n",
      "single token. Thus the input to layer norm is a single vector of dimensionality d\n",
      "and the output is that vector normalized, again of dimensionality d. The ﬁrst step in\n",
      "layer normalization is to calculate the mean, µ, and standard deviation, σ, over the\n",
      "elements of the vector to be normalized. Given an embedding vector x of dimen-\n",
      "sionality d, these values are calculated as follows.\n",
      "µ = 1\n",
      "d\n",
      "d∑\n",
      "i=1\n",
      "xi (9.22)\n",
      "σ =\n",
      "√1\n",
      "d\n",
      "d∑\n",
      "i=1\n",
      "(xi −µ)2 (9.23)\n",
      "Given these values, the vector components are normalized by subtracting the mean\n",
      "from each and dividing by the standard deviation. The result of this computation is\n",
      "a new vector with zero mean and a standard deviation of one.\n",
      "ˆ x= (x−µ)\n",
      "σ (9.24)\n",
      "Finally, in the standard implementation of layer normalization, two learnable param-\n",
      "eters, γ and β, representing gain and offset values, are introduced.\n",
      "LayerNorm(x) =γ (x−µ)\n",
      "σ +β (9.25)\n",
      "Putting it all together The function computed by a transformer block can be ex-\n",
      "pressed by breaking it down with one equation for each component computation,\n",
      "using t (of shape [1 ×d]) to stand for transformer and superscripts to demarcate\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 8, 'page_label': '9'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 10 CHAPTER 9 • T HE TRANSFORMER\n",
      "each computation inside the block:\n",
      "t1\n",
      "i = LayerNorm(xi) (9.26)\n",
      "t2\n",
      "i = MultiHeadAttention(t1\n",
      "i ,\n",
      "[\n",
      "t1\n",
      "1,···,t1\n",
      "N\n",
      "]\n",
      ") (9.27)\n",
      "t3\n",
      "i = t2\n",
      "i +xi (9.28)\n",
      "t4\n",
      "i = LayerNorm(t3\n",
      "i ) (9.29)\n",
      "t5\n",
      "i = FFN(t4\n",
      "i ) (9.30)\n",
      "hi = t5\n",
      "i +t3\n",
      "i (9.31)\n",
      "Notice that the only component that takes as input information from other tokens\n",
      "(other residual streams) is multi-head attention, which (as we see from Eq. 9.28)\n",
      "looks at all the neighboring tokens in the context. The output from attention, how-\n",
      "ever, is then added into this token’s embedding stream. In fact, Elhage et al. (2021)\n",
      "show that we can view attention heads as literally moving information from the\n",
      "residual stream of a neighboring token into the current stream. The high-dimensional\n",
      "embedding space at each position thus contains information about the current to-\n",
      "ken and about neighboring tokens, albeit in different subspaces of the vector space.\n",
      "Fig. 9.7 shows a visualization of this movement.\n",
      "Token A\n",
      "residual\n",
      " stream\n",
      "Token B\n",
      "residual \n",
      "stream\n",
      "Figure 9.7 An attention head can move information from token A’s residual stream into\n",
      "token B’s residual stream.\n",
      "Crucially, the input and output dimensions of transformer blocks are matched so\n",
      "they can be stacked. Each token vectorxi at the input to the block has dimensionality\n",
      "d, and the output hi also has dimensionality d. Transformers for large language\n",
      "models stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small\n",
      "language models) to 96 layers (used for GPT-3 large), to even more for more recent\n",
      "models. We’ll come back to this issue of stacking in a bit.\n",
      "Equation 9.28 and following are just the equation for a single transformer block,\n",
      "but the residual stream metaphor goes through all the transformer layers, from the\n",
      "ﬁrst transformer blocks to the 12th, in a 12-layer transformer. At the earlier trans-\n",
      "former blocks, the residual stream is representing the current token. At the highest\n",
      "transformer blocks, the residual stream is usually representing the following token,\n",
      "since at the very end it’s being trained to predict the next token.\n",
      "Once we stack many blocks, there is one more requirement: at the very end of\n",
      "the last (highest) transformer block, there is a single extra layer norm that is run on\n",
      "the last hi of each token stream (just below the language model head layer that we\n",
      "will deﬁne soon). 3\n",
      "3 Note that we are using the most common current transformer architecture, which is called theprenorm\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 9, 'page_label': '10'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.3 • P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 11\n",
      "9.3 Parallelizing computation using a single matrix X\n",
      "This description of multi-head attention and the rest of the transformer block has\n",
      "been from the perspective of computing a single output at a single time step i in\n",
      "a single residual stream. But as we pointed out earlier, the attention computation\n",
      "performed for each token to compute ai is independent of the computation for each\n",
      "other token, and that’s also true for all the computation in the transformer block\n",
      "computing hi from the input xi. That means we can easily parallelize the entire\n",
      "computation, taking advantage of efﬁcient matrix multiplication routines.\n",
      "We do this by packing the input embeddings for the N tokens of the input se-\n",
      "quence into a single matrix X of size [N ×d]. Each row of X is the embedding of\n",
      "one token of the input. Transformers for large language models commonly have an\n",
      "input length N from 1K to 32K; much longer contexts of 128K or even up to millions\n",
      "of tokens can also be achieved with architectural changes like special long-context\n",
      "mechanisms that we don’t discuss here. So for vanilla transformers, we can think of\n",
      "X having between 1K and 32K rows, each of the dimensionality of the embedding\n",
      "d (the model dimension).\n",
      "Parallelizing attention Let’s ﬁrst see this for a single attention head and then turn\n",
      "to multiple heads, and then add in the rest of the components in the transformer\n",
      "block. For one head we multiply X by the key, query, and value matrices WQ of\n",
      "shape [d ×dk], WK of shape [d ×dk], and WV of shape [d ×dv], to produce matrices\n",
      "Q of shape [N ×dk], K of shape [N ×dk], and V of shape [N ×dv], containing all the\n",
      "key, query, and value vectors:\n",
      "Q = XWQ; K = XWK; V = XWV (9.32)\n",
      "Given these matrices we can compute all the requisite query-key comparisons simul-\n",
      "taneously by multiplying Q and K⊺ in a single matrix multiplication. The product is\n",
      "of shape N ×N, visualized in Fig. 9.8.\n",
      "q1•k1\n",
      "q2•k1 q2•k2\n",
      "q4•k1 q4•k2 q4•k3 q4•k4\n",
      "q3•k1 q3•k2 q3•k3\n",
      "N\n",
      "N\n",
      "q1•k2 q1•k3 q1•k4\n",
      "q2•k3 q2•k4\n",
      "q3•k4\n",
      "Figure 9.8 The N ×N QK⊺ matrix showing how it computes all qi ·kj comparisons in a\n",
      "single matrix multiple.\n",
      "Once we have this QK⊺ matrix, we can very efﬁciently scale these scores, take\n",
      "the softmax, and then multiply the result by V resulting in a matrix of shape N ×d:\n",
      "a vector embedding representation for each token in the input. We’ve reduced the\n",
      "entire self-attention step for an entire sequence of N tokens for one head to the\n",
      "architecture. The original deﬁnition of the transformer in Vaswani et al. (2017) used an alternative archi-\n",
      "tecture called the postnorm transformer in which the layer norm happens after the attention and FFN\n",
      "layers; it turns out moving the layer norm beforehand works better, but does require this one extra layer\n",
      "at the end.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 10, 'page_label': '11'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 12 CHAPTER 9 • T HE TRANSFORMER\n",
      "following computation:\n",
      "head = softmax\n",
      "(\n",
      "mask\n",
      "(QK⊺\n",
      "√dk\n",
      "))\n",
      "V\n",
      "A = head WO (9.33)\n",
      "Masking out the future You may have noticed that we introduced a mask function\n",
      "in Eq. 9.33 above. This is because the self-attention computation as we’ve described\n",
      "it has a problem: the calculation of QK⊺ results in a score for each query value to\n",
      "every key value, including those that follow the query . This is inappropriate in the\n",
      "setting of language modeling: guessing the next word is pretty simple if you already\n",
      "know it! To ﬁx this, the elements in the upper-triangular portion of the matrix are set\n",
      "to −∞, which the softmax will turn to zero, thus eliminating any knowledge of words\n",
      "that follow in the sequence. This is done in practice by adding a mask matrix M in\n",
      "which Mi j = −∞ ∀j > i (i.e. for the upper-triangular portion) andMi j = 0 otherwise.\n",
      "Fig. 9.9 shows the resulting masked QK⊺ matrix. (we’ll see in Chapter 11 how to\n",
      "make use of words in the future for tasks that need it).\n",
      "q1•k1\n",
      "q2•k1 q2•k2\n",
      "q4•k1 q4•k2 q4•k3 q4•k4\n",
      "q3•k1 q3•k2 q3•k3\n",
      "N\n",
      "N\n",
      "−∞ −∞\n",
      "−∞ −∞\n",
      "−∞\n",
      "−∞\n",
      "Figure 9.9 The N ×N QK⊺ matrix showing the qi ·kj values, with the upper-triangle por-\n",
      "tion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero).\n",
      "Fig. 9.10 shows a schematic of all the computations for a single attention head\n",
      "parallelized in matrix form.\n",
      "Fig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length\n",
      "of the input, since at each layer we need to compute dot products between each pair\n",
      "of tokens in the input. This makes it expensive to compute attention over very long\n",
      "documents (like entire novels). Nonetheless modern large language models manage\n",
      "to use quite long contexts of thousands or tens of thousands of tokens.\n",
      "Parallelizing multi-head attention In multi-head attention, as with self-attention,\n",
      "the input and output have the model dimension d, the key and query embeddings\n",
      "have dimensionality dk, and the value embeddings are of dimensionality dv (again,\n",
      "in the original transformer paper dk = dv = 64, A = 8, and d = 512). Thus for\n",
      "each head i, we have weight layers WQi of shape [d ×dk], WKi of shape [d ×dk],\n",
      "and WVi of shape [d ×dv], and these get multiplied by the inputs packed into X\n",
      "to produce Q of shape [N ×dk], K of shape [N ×dk], and V of shape [N ×dv].\n",
      "The output of each of the A heads is of shape N ×dv, and so the output of the\n",
      "multi-head layer with A heads consists of A matrices of shape N ×dv. To make use\n",
      "of these matrices in further processing, they are concatenated to produce a single\n",
      "output with dimensionality N ×hdv. Finally, we use a ﬁnal linear projection WO\n",
      "of shape [Adv ×d], that reshape it to the original output dimension for each token.\n",
      "Multiplying the concatenatedN ×hdv matrix output byWO of shape [Adv ×d] yields\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 11, 'page_label': '12'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.3 • P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 13\n",
      "q1\n",
      "q2\n",
      "q3\n",
      "q4\n",
      "k1\n",
      "k2\n",
      "k3\n",
      "k4\n",
      "Q KT\n",
      "QKT\n",
      "v1\n",
      "v2\n",
      "v3\n",
      "v4\n",
      "V\n",
      "q2•k2\n",
      "q4•k2 q4•k3 q4•k4\n",
      "q3•k2 q3•k3\n",
      "−∞ −∞\n",
      "−∞ −∞\n",
      "−∞\n",
      "−∞q1•k1\n",
      "q2•k1 q2•k2\n",
      "q4•k1 q4•k2 q4•k3 q4•k4\n",
      "q3•k1 q3•k2 q3•k3\n",
      "q1•k2\n",
      "q2•k3\n",
      "q1•k3\n",
      "q3•k4\n",
      "q2•k4\n",
      "q1•k4x =\n",
      "QKT masked\n",
      "mask =\n",
      "q1•k1\n",
      "q2•k1\n",
      "q4•k1\n",
      "q3•k1\n",
      "q1•k1q1•k1\n",
      "=x\n",
      "a1\n",
      "a2\n",
      "a3\n",
      "a4\n",
      "A\n",
      "Query \n",
      "Token 1\n",
      "Query \n",
      "Token 2\n",
      "Query \n",
      "Token 3\n",
      "Query \n",
      "Token 4\n",
      "Q\n",
      "Input \n",
      "Token 1\n",
      "Input \n",
      "Token 2\n",
      "Input \n",
      "Token 3\n",
      "Input \n",
      "Token 4\n",
      "X\n",
      "x\n",
      "WQ\n",
      "=\n",
      "Value \n",
      "Token 1\n",
      "Value \n",
      "Token 2\n",
      "Value \n",
      "Token 3\n",
      "Value \n",
      "Token 4\n",
      "V\n",
      "x\n",
      "WV\n",
      "=\n",
      "Input \n",
      "Token 1\n",
      "Input \n",
      "Token 2\n",
      "Input \n",
      "Token 3\n",
      "Input \n",
      "Token 4\n",
      "X\n",
      "Key \n",
      "Token 1\n",
      "Key \n",
      "Token 2\n",
      "Key \n",
      "Token 3\n",
      "Key \n",
      "Token 4\n",
      "K\n",
      "x\n",
      "WK\n",
      "=\n",
      "Input \n",
      "Token 1\n",
      "Input \n",
      "Token 2\n",
      "Input \n",
      "Token 3\n",
      "Input \n",
      "Token 4\n",
      "X\n",
      "N x dk\n",
      "dk x N\n",
      "N x N N x N N x dv N x dv\n",
      "d x dk\n",
      "d x dk d x dv\n",
      "N x d N x dk N x d N x dk N x d N x dv\n",
      "Figure 9.10 Schematic of the attention computation for a single attention head in parallel. The ﬁrst row shows\n",
      "the computation of the Q, K, and V matrices. The second row shows the computation of QKT, the masking\n",
      "(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of\n",
      "the value vectors to get the ﬁnal attention vectors.\n",
      "the self-attention output A of shape [N ×d].\n",
      "Qi = XWQi ; Ki = XWKi ; Vi = XWVi (9.34)\n",
      "headi = SelfAttention(Qi,Ki,Vi) = softmax\n",
      "(QiKi⊺\n",
      "√dk\n",
      ")\n",
      "Vi (9.35)\n",
      "MultiHeadAttention(X) = (head1 ⊕head2...⊕headA)WO (9.36)\n",
      "Putting it all together with the parallel input matrix X The function computed\n",
      "in parallel by an entire layer of N transformer block over the entire N input tokens\n",
      "can be expressed as:\n",
      "O = X+MultiHeadAttention(LayerNorm(X)) (9.37)\n",
      "H = O+FFN(LayerNorm(O)) (9.38)\n",
      "Note that in Eq. 9.37 we are using X to mean the input to the layer, wherever it\n",
      "comes from. For the ﬁrst layer, as we will see in the next section, that input is the\n",
      "initital word + positional embedding vectors that we have been describing byX. But\n",
      "for subsequent layers k, the input is the output from the previous layer Hk−1. We\n",
      "can also break down the computation performed in a transformer layer, showing one\n",
      "equation for each component computation. We’ll use T (of shape [N ×d]) to stand\n",
      "for transformer and superscripts to demarcate each computation inside the block,\n",
      "and again use X to mean the input to the block from the previous layer or the initial\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 12, 'page_label': '13'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 14 CHAPTER 9 • T HE TRANSFORMER\n",
      "embedding:\n",
      "T1 = LayerNorm(X) (9.39)\n",
      "T2 = MultiHeadAttention(T1) (9.40)\n",
      "T3 = T2 +X (9.41)\n",
      "T4 = LayerNorm(T3) (9.42)\n",
      "T5 = FFN(T4) (9.43)\n",
      "H = T5 +T3 (9.44)\n",
      "Here when we use a notation like FFN (T3) we mean that the same FFN is applied\n",
      "in parallel to each of the N embedding vectors in the window. Similarly, each of the\n",
      "N tokens is normed in parallel in the LayerNorm. Crucially, the input and output\n",
      "dimensions of transformer blocks are matched so they can be stacked. Since each\n",
      "token xi at the input to the block is represented by an embedding of dimensionality\n",
      "[1 ×d], that means the input X and output H are both of shape [N ×d].\n",
      "9.4 The input: embeddings for token and position\n",
      "Let’s talk about where the inputX comes from. Given a sequence of N tokens (N is\n",
      "the context length in tokens), the matrix X of shape [N ×d] has an embedding forembedding\n",
      "each word in the context. The transformer does this by separately computing two\n",
      "embeddings: an input token embedding, and an input positional embedding.\n",
      "A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-\n",
      "mension d that will be our initial representation for the input token. (As we pass\n",
      "vectors up through the transformer layers in the residual stream, this embedding\n",
      "representation will change and grow, incorporating context and playing a different\n",
      "role depending on the kind of language model we are building.) The set of initial\n",
      "embeddings are stored in the embedding matrix E, which has a row for each of the\n",
      "|V |tokens in the vocabulary. Thus each word is a row vector of d dimensions, and\n",
      "E has shape [|V |×d].\n",
      "Given an input token string like Thanks for all the we ﬁrst convert the tokens\n",
      "into vocabulary indices (these were created when we ﬁrst tokenized the input using\n",
      "BPE or SentencePiece). So the representation of thanks for all the might be w =\n",
      "[5,4000,10532,2224]. Next we use indexing to select the corresponding rows from\n",
      "E, (row 5, row 4000, row 10532, row 2224).\n",
      "Another way to think about selecting token embeddings from the embedding\n",
      "matrix is to represent tokens as one-hot vectors of shape [1 ×|V |], i.e., with one\n",
      "dimension for each word in the vocabulary. Recall that in a one-hot vector all theone-hot vector\n",
      "elements are 0 except one, the element whose dimension is the word’s index in the\n",
      "vocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,\n",
      "x5 = 1, and xi = 0 ∀i ̸= 5, as shown here:\n",
      "[0 0 0 0 1 0 0 ... 0 0 0 0]\n",
      "1 2 3 4 5 6 7 ... ... |V|\n",
      "Multiplying by a one-hot vector that has only one non-zero elementxi = 1 simply\n",
      "selects out the relevant row vector for wordi, resulting in the embedding for word i,\n",
      "as depicted in Fig. 9.11.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 13, 'page_label': '14'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.4 • T HE INPUT : EMBEDDINGS FOR TOKEN AND POSITION 15\n",
      "E\n",
      "|V|\n",
      "d\n",
      "1\n",
      "|V| d\n",
      "=✕\n",
      "55\n",
      "0 0 0 0 1 0 0 … 0 0 0 0 1\n",
      "Figure 9.11 Selecting the embedding vector for word V5 by multiplying the embedding\n",
      "matrix E with a one-hot vector with a 1 in index 5.\n",
      "We can extend this idea to represent the entire token sequence as a matrix of one-\n",
      "hot vectors, one for each of the N positions in the transformer’s context window, as\n",
      "shown in Fig. 9.12.\n",
      "E\n",
      "|V|\n",
      "d\n",
      "d\n",
      "N\n",
      "=✕\n",
      "|V|\n",
      "N\n",
      "0 0 0 0 0 0 0 … 0 0 1 0 \n",
      "0 0 0 0 1 0 0 … 0 0 0 0 \n",
      "1 0 0 0 0 0 0 … 0 0 0 0 \n",
      "0 0 0 0 1 0 0 … 0 0 0 0 \n",
      "…\n",
      "Figure 9.12 Selecting the embedding matrix for the input sequence of token idsW by mul-\n",
      "tiplying a one-hot matrix corresponding to W by the embedding matrix E.\n",
      "These token embeddings are not position-dependent. To represent the position\n",
      "of each token in the sequence, we combine these token embeddings with positional\n",
      "embeddings speciﬁc to each position in an input sequence.positional\n",
      "embeddings\n",
      "Where do we get these positional embeddings? The simplest method, called\n",
      "absolute position, is to start with randomly initialized embeddings correspondingabsolute\n",
      "position\n",
      "to each possible input position up to some maximum length. For example, just as\n",
      "we have an embedding for the wordﬁsh, we’ll have an embedding for the position 3.\n",
      "As with word embeddings, these positional embeddings are learned along with other\n",
      "parameters during training. We can store them in a matrix Epos of shape [N ×d].\n",
      "To produce an input embedding that captures positional information, we just\n",
      "add the word embedding for each input to its corresponding positional embedding.\n",
      "The individual token and position embeddings are both of size[1×d], so their sum is\n",
      "also [1×d], This new embedding serves as the input for further processing. Fig. 9.13\n",
      "shows the idea.\n",
      "X = Composite\n",
      "Embeddings\n",
      "(word + position)\n",
      "Transformer Block\n",
      "Janet\n",
      "1\n",
      "will\n",
      "2\n",
      "back\n",
      "3\n",
      "Janet will back the bill\n",
      "the\n",
      "4\n",
      "bill\n",
      "5\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "Position\n",
      "Embeddings\n",
      "Word\n",
      "Embeddings\n",
      "Figure 9.13 A simple way to model position: add an embedding of the absolute position to\n",
      "the token embedding to produce a new embedding of the same dimensionality.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 14, 'page_label': '15'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 16 CHAPTER 9 • T HE TRANSFORMER\n",
      "The ﬁnal representation of the input, the matrix X, is an [N ×d] matrix in which\n",
      "each row i is the representation of the ith token in the input, computed by adding\n",
      "E[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i],\n",
      "the positional embedding of position i.\n",
      "A potential problem with the simple position embedding approach is that there\n",
      "will be plenty of training examples for the initial positions in our inputs and corre-\n",
      "spondingly fewer at the outer length limits. These latter embeddings may be poorly\n",
      "trained and may not generalize well during testing. An alternative is to choose a\n",
      "static function that maps integer inputs to real-valued vectors in a way that better\n",
      "handle sequences of arbitrary length. A combination of sine and cosine functions\n",
      "with differing frequencies was used in the original transformer work. Sinusoidal po-\n",
      "sition embeddings may also help in capturing the inherent relationships among the\n",
      "positions, like the fact that position 4 in an input is more closely related to position\n",
      "5 than it is to position 17.\n",
      "A more complex style of positional embedding methods extend this idea of cap-\n",
      "turing relationships even further to directly represent relative position instead ofrelative\n",
      "position\n",
      "absolute position, often implemented in the attention mechanism at each layer rather\n",
      "than being added once at the initial input.\n",
      "9.5 The Language Modeling Head\n",
      "The last component of the transformer we must introduce is thelanguage modeling\n",
      "head. Here we are using the word head to mean the additional neural circuitry welanguage\n",
      "modeling head\n",
      "head add on top of the basic transformer architecture when we apply pretrained trans-\n",
      "former models to various tasks. The language modeling head is the circuitry we\n",
      "need to do language modeling.\n",
      "Recall that language models, from the simple n-gram models of Chapter 3 through\n",
      "the feedforward and RNN language models of Chapter 7 and Chapter 8, are word\n",
      "predictors. Given a context of words, they assign a probability to each possible next\n",
      "word. For example, if the preceding context is “Thanks for all the” and we want to\n",
      "know how likely the next word is “ﬁsh” we would compute:\n",
      "P(ﬁsh|Thanks for all the)\n",
      "Language models give us the ability to assign such a conditional probability to every\n",
      "possible next word, giving us a distribution over the entire vocabulary. The n-gram\n",
      "language models of Chapter 3 compute the probability of a word given counts of\n",
      "its occurrence with the n −1 prior words. The context is thus of size n −1. For\n",
      "transformer language models, the context is the size of the transformer’s context\n",
      "window, which can be quite large, like 32K tokens for large models (and much larger\n",
      "contexts of millions of words are possible with special long-context architectures).\n",
      "The job of the language modeling head is to take the output of the ﬁnal trans-\n",
      "former layer from the last token N and use it to predict the upcoming word at posi-\n",
      "tion N +1. Fig. 9.14 shows how to accomplish this task, taking the output of the last\n",
      "token at the last layer (the d-dimensional output embedding of shape [1 ×d]) and\n",
      "producing a probability distribution over words (from which we will choose one to\n",
      "generate).\n",
      "The ﬁrst module in Fig. 9.14 is a linear layer, whose job is to project from the\n",
      "output hL\n",
      "N , which represents the output token embedding at positionN from the ﬁnal\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 15, 'page_label': '16'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 9.5 • T HE LANGUAGE MODELING HEAD 17\n",
      "Layer L\n",
      "Transformer\n",
      "Block\n",
      "Softmax over vocabulary V\n",
      "Unembedding layer\n",
      "…\n",
      "1 x |V|\n",
      "Logits \n",
      "Word probabilities\n",
      "1 x |V|\n",
      "hL\n",
      "1\n",
      "w1 w2 wN\n",
      "hL\n",
      "2 hL\n",
      "N\n",
      "d x |V|\n",
      "1 x d\n",
      "   Unembedding layer\n",
      "U = ET\n",
      "y1 y2 y|V|…\n",
      "u1 u2 u|V|…\n",
      "Language Model Head\n",
      "takes hL\n",
      "N and outputs a\n",
      "distribution over vocabulary V\n",
      "Figure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output\n",
      "embedding for token N from the last transformer layer ( hL\n",
      "N ) to a probability distribution over words in the\n",
      "vocabulary V .\n",
      "block L, (hence of shape [1 ×d]) to the logit vector, or score vector, that will have alogit\n",
      "single score for each of the |V |possible words in the vocabularyV . The logit vector\n",
      "u is thus of dimensionality 1 ×|V |.\n",
      "This linear layer can be learned, but more commonly we tie this matrix to (the\n",
      "transpose of) the embedding matrix E. Recall that in weight tying , we use theweight tying\n",
      "same weights for two different matrices in the model. Thus at the input stage of the\n",
      "transformer the embedding matrix (of shape[|V |×d]) is used to map from a one-hot\n",
      "vector over the vocabulary (of shape [1 ×|V |]) to an embedding (of shape [1 ×d]).\n",
      "And then in the language model head,ET, the transpose of the embedding matrix (of\n",
      "shape [d ×|V |]) is used to map back from an embedding (shape [1 ×d]) to a vector\n",
      "over the vocabulary (shape [1×|V |]). In the learning process, E will be optimized to\n",
      "be good at doing both of these mappings. We therefore sometimes call the transpose\n",
      "ET the unembedding layer because it is performing this reverse mapping.unembedding\n",
      "A softmax layer turns the logits u into the probabilities y over the vocabulary.\n",
      "u = hL\n",
      "N ET (9.45)\n",
      "y = softmax(u) (9.46)\n",
      "We can use these probabilities to do things like help assign a probability to a\n",
      "given text. But the most important usage to generate text, which we do bysampling\n",
      "a word from these probabilities y. We might sample the highest probability word\n",
      "(‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Sec-\n",
      "tion ??. In either case, whatever entry yk we choose from the probability vector y,\n",
      "we generate the word that has that index k.\n",
      "Fig. 9.15 shows the total stacked architecture for one tokeni. Note that the input\n",
      "to each transformer layer xℓ\n",
      "i is the same as the output from the preceding layer hℓ−1\n",
      "i .\n",
      "Now that we see all these transformer layers spread out on the page, we can point\n",
      "out another useful feature of the unembedding layer: as a tool for interpretability of\n",
      "the internals of the transformer that we call the logit lens (Nostalgebraist, 2020).logit lens\n",
      "We can take a vector from any layer of the transformer and, pretending that it is\n",
      "the preﬁnal embedding, simply multiply it by the unembedding layer to get logits,\n",
      "and compute a softmax to see the distribution over words that that vector might\n",
      "be representing. This can be a useful window into the internal representations of\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 16, 'page_label': '17'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 18 CHAPTER 9 • T HE TRANSFORMER\n",
      "wi\n",
      "Sample token to\n",
      "generate at position i+1\n",
      "feedforward\n",
      "layer norm\n",
      "attention\n",
      "layer norm\n",
      "U\n",
      "Input token\n",
      "Language\n",
      "Modeling\n",
      "Head\n",
      "Input\n",
      "Encoding\n",
      " E\n",
      "i+\n",
      "…\n",
      "logits\n",
      "feedforward\n",
      "layer norm\n",
      "attention\n",
      "layer norm\n",
      "Layer 1\n",
      "Layer 2\n",
      "h1\n",
      "i  =  x2\n",
      "i\n",
      "x1\n",
      "i\n",
      "h2\n",
      "i  =  x3\n",
      "i\n",
      "feedforward\n",
      "layer norm\n",
      "attention\n",
      "layer norm\n",
      "hL\n",
      "i  \n",
      "hL-1\n",
      "i  =  xL\n",
      "i\n",
      "y1 y2 y|V|…Token probabilities\n",
      "u1 u2 u|V|…\n",
      "softmax\n",
      "wi+1\n",
      "Layer L\n",
      "Figure 9.15 A transformer language model (decoder-only), stacking transformer blocks\n",
      "and mapping from an input token wi to to a predicted next token wi+1.\n",
      "the model. Since the network wasn’t trained to make the internal representations\n",
      "function in this way, the logit lens doesn’t always work perfectly, but this can still\n",
      "be a useful trick.\n",
      "A terminological note before we conclude: You will sometimes see a trans-\n",
      "former used for this kind of unidirectional causal language model called a decoder-\n",
      "only model. This is because this model constitutes roughly half of the encoder-decoder-only\n",
      "model\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 17, 'page_label': '18'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: BIBLIOGRAPHICAL AND HISTORICAL NOTES 19\n",
      "Here’s a summary of the main points that we covered:\n",
      "• Transformers are non-recurrent networks based on multi-head attention, a\n",
      "kind of self-attention. A multi-head attention computation takes an input\n",
      "vector xi and maps it to an output ai by adding in vectors from prior tokens,\n",
      "weighted by how relevant they are for the processing of the current word.\n",
      "• A transformer block consists of a residual stream in which the input from\n",
      "the prior layer is passed up to the next layer, with the output of different com-\n",
      "ponents added to it. These components include a multi-head attention layer\n",
      "followed by a feedforward layer, each preceded by layer normalizations.\n",
      "Transformer blocks are stacked to make deeper and more powerful networks.\n",
      "• The input to a transformer is computed by adding an embedding (computed\n",
      "with an embedding matrix) to a positional encoding that represents the se-\n",
      "quential position of the token in the window.\n",
      "• Language models can be built out of stacks of transformer blocks, with a\n",
      "language model head at the top, which applies an unembedding matrix to\n",
      "the output H of the top layer to generate the logits, which are then passed\n",
      "through a softmax to generate word probabilities.\n",
      "• Transformer-based language models have a wide context window (200K to-\n",
      "kens or even more for very large models with special mechanisms) allowing\n",
      "them to draw on enormous amounts of context to predict upcoming words.\n",
      "Bibliographical and Historical Notes\n",
      "The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\n",
      "research: self-attention and memory networks.\n",
      "Encoder-decoder attention, the idea of using a soft weighting over the encodings\n",
      "of input words to inform a generative decoder (see Chapter 13) was developed by\n",
      "Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\n",
      "for MT. This idea was extended to self-attention by dropping the need for separate\n",
      "encoding and decoding sequences and instead seeing attention as a way of weighting\n",
      "the tokens in collecting information passed from lower layers to higher layers (Ling\n",
      "et al., 2015; Cheng et al., 2016; Liu et al., 2016).\n",
      "Other aspects of the transformer, including the terminology of key, query, and\n",
      "value, came from memory networks, a mechanism for adding an external read-\n",
      "write memory to networks, by using an embedding of a query to match keys rep-\n",
      "resenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,\n",
      "2015; Graves et al., 2014).\n",
      "MORE HISTORY TBD IN NEXT DRAFT.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "<><><><><><><><><><><><><><><><><><><>\n",
      "Content: 20 Chapter 9 • The Transformer\n",
      "Ba, J. L., J. R. Kiros, and G. E. Hinton. 2016. Layer normal-\n",
      "ization. NeurIPS workshop.\n",
      "Bahdanau, D., K. H. Cho, and Y . Bengio. 2015. Neural ma-\n",
      "chine translation by jointly learning to align and translate.\n",
      "ICLR 2015.\n",
      "Cheng, J., L. Dong, and M. Lapata. 2016. Long short-term\n",
      "memory-networks for machine reading. EMNLP.\n",
      "Elhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\n",
      "B. Mann, A. Askell, Y . Bai, A. Chen, T. Conerly, N. Das-\n",
      "Sarma, D. Drain, D. Ganguli, Z. Hatﬁeld-Dodds, D. Her-\n",
      "nandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\n",
      "D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-\n",
      "dlish, and C. Olah. 2021. A mathematical framework for\n",
      "transformer circuits. White paper.\n",
      "Graves, A. 2013. Generating sequences with recurrent neural\n",
      "networks. ArXiv.\n",
      "Graves, A., G. Wayne, and I. Danihelka. 2014. Neural Tur-\n",
      "ing machines. ArXiv.\n",
      "Ling, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\n",
      "S. Amir, L. Marujo, and T. Lu´ıs. 2015. Finding function\n",
      "in form: Compositional character models for open vocab-\n",
      "ulary word representation. EMNLP.\n",
      "Liu, Y ., C. Sun, L. Lin, and X. Wang. 2016. Learning natural\n",
      "language inference using bidirectional LSTM model and\n",
      "inner-attention. ArXiv.\n",
      "Nostalgebraist. 2020. Interpreting gpt: the logit lens. White\n",
      "paper.\n",
      "Sukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015.\n",
      "End-to-end memory networks. NeurIPS.\n",
      "Uszkoreit, J. 2017. Transformer: A novel neural network ar-\n",
      "chitecture for language understanding. Google Research\n",
      "blog post, Thursday August 31, 2017.\n",
      "Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\n",
      "A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten-\n",
      "tion is all you need. NeurIPS.\n",
      "Weston, J., S. Chopra, and A. Bordes. 2015. Memory net-\n",
      "works. ICLR 2015.\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-12T12:07:39-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'total_pages': 20, 'page': 19, 'page_label': '20'}\n",
      "<><><><><><><><><><><><><><><><><><><>\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(\"<><><><><><><><><><><><><><><><><><><>\")\n",
    "    print(\"Content:\", doc.page_content)\n",
    "    print(\"<-><-><-><-><-><-><-><-><-><-><-><-><->\")\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"<><><><><><><><><><><><><><><><><><><>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[5].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2024-04-10T21:11:43+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2024-04-10T21:11:43+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf',\n",
       " 'total_pages': 15,\n",
       " 'page': 2,\n",
       " 'page_label': '3'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[5].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(len(texts)):\\n    print(texts[i].metadata[\"moddate\"])'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for i in range(len(texts)):\n",
    "    print(texts[i].metadata[\"moddate\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.21',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2025-01-12T12:07:39-08:00',\n",
       " 'author': '',\n",
       " 'title': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2025-01-12T12:07:39-08:00',\n",
       " 'trapped': '/False',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       " 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf',\n",
       " 'total_pages': 20,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[27].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db'\n",
    "\n",
    "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"what is attention?what is encoder?\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 5}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming 'retriever' is already defined\n",
    "retriever = vectordb.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention is a mechanism used in neural networks to selectively focus on certain parts of the input that are relevant for the task at hand. It can be used to weight the input tokens and collect information passed from lower layers to higher layers. An encoder is a component of a neural network that takes in the input data and converts it into a form that can be used by the rest of the network. In the context of self-attention and memory networks, an encoder is responsible for encoding the input tokens and passing them to the decoder or higher layers.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf\n",
      "/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf\n",
      "/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf\n",
      "/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf\n"
     ]
    }
   ],
   "source": [
    "# full example\n",
    "query = \"What is attention?.What is encoder?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'list the important points of the documents ',\n",
       " 'result': '\\n1. Contextual words can help us understand the meaning of words in a sentence or paragraph.\\n2. Transformers can build contextual embeddings by integrating the meaning of these helpful contextual words.\\n3. Attention is the mechanism in transformers that weighs and combines the representations of other tokens to build the representation for a specific token.\\n4. Attention can be seen as a causal language model, processing the sentence from left to right.\\n5. Words have rich linguistic relationships with other words, even if they are far away in the text.\\n6. Attention can follow long-distance dependencies to complete phrases or concepts.\\n7. A majority of American governments have passed laws since 2009 to make the registration or voting process more difficult.',\n",
       " 'source_documents': [Document(metadata={'author': '', 'creationdate': '2025-01-12T12:07:39-08:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='9.1 • A TTENTION 3\\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\\nlanguages, must agree in grammatical number with the verbare; in this case both are\\nplural. In English we can’t use a singular verb like is with a plural subject like keys\\n(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\\nto the side of a pond or river and not a ﬁnancial institution because of the context,\\nincluding words like pond. (We’ll discuss word senses more in Chapter 11.)\\nThe point of all these examples is that these contextual words that help us com-\\npute the meaning of words in context can be quite far away in the sentence or para-\\ngraph. Transformers can build contextual representations of word meaning, contex-\\ntual embeddings, by integrating the meaning of these helpful contextual words. In acontextual\\nembeddings\\ntransformer, layer by layer, we build up richer and richer contextualized representa-'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2025-01-12T12:07:39-08:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='9.1 • A TTENTION 3\\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\\nlanguages, must agree in grammatical number with the verbare; in this case both are\\nplural. In English we can’t use a singular verb like is with a plural subject like keys\\n(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\\nto the side of a pond or river and not a ﬁnancial institution because of the context,\\nincluding words like pond. (We’ll discuss word senses more in Chapter 11.)\\nThe point of all these examples is that these contextual words that help us com-\\npute the meaning of words in context can be quite far away in the sentence or para-\\ngraph. Transformers can build contextual representations of word meaning, contex-\\ntual embeddings, by integrating the meaning of these helpful contextual words. In acontextual\\nembeddings\\ntransformer, layer by layer, we build up richer and richer contextualized representa-\\ntions of the meanings of input tokens. At each layer, we compute the representation\\nof a token i by combining information about i from the previous layer with infor-\\nmation about the neighboring tokens to produce a contextualized representation for\\neach word at each position.\\nAttention is the mechanism in the transformer that weighs and combines the\\nrepresentations from appropriate other tokens in the context from layerk−1 to build\\nthe representation for tokens in layer k.\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nLayer k+1\\nLayer k\\nself-attention distribution\\ncolumns corresponding to input tokens\\nFigure 9.2 The self-attention weight distribution α that is part of the computation of the\\nrepresentation for the word it at layer k +1. In computing the representation for it, we attend\\ndifferently to the various words at layer k, with darker shades indicating higher self-attention'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2025-01-12T12:07:39-08:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='9.1 • A TTENTION 3\\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\\nlanguages, must agree in grammatical number with the verbare; in this case both are\\nplural. In English we can’t use a singular verb like is with a plural subject like keys\\n(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\\nto the side of a pond or river and not a ﬁnancial institution because of the context,\\nincluding words like pond. (We’ll discuss word senses more in Chapter 11.)\\nThe point of all these examples is that these contextual words that help us com-\\npute the meaning of words in context can be quite far away in the sentence or para-\\ngraph. Transformers can build contextual representations of word meaning, contex-\\ntual embeddings, by integrating the meaning of these helpful contextual words. In acontextual\\nembeddings\\ntransformer, layer by layer, we build up richer and richer contextualized representa-\\ntions of the meanings of input tokens. At each layer, we compute the representation\\nof a token i by combining information about i from the previous layer with infor-\\nmation about the neighboring tokens to produce a contextualized representation for\\neach word at each position.\\nAttention is the mechanism in the transformer that weighs and combines the\\nrepresentations from appropriate other tokens in the context from layerk−1 to build\\nthe representation for tokens in layer k.\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nLayer k+1\\nLayer k\\nself-attention distribution\\ncolumns corresponding to input tokens\\nFigure 9.2 The self-attention weight distribution α that is part of the computation of the\\nrepresentation for the word it at layer k +1. In computing the representation for it, we attend\\ndifferently to the various words at layer k, with darker shades indicating higher self-attention'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2025-01-12T12:07:39-08:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='Furthermore, consider reading left to right like a causal language model, pro-\\ncessing the sentence up to the word it:\\n(9.3) The chicken didn’t cross the road becauseit\\nAt this point we don’t yet know which thingit is going to end up referring to! So a\\nrepresentation of it at this point might have aspects of both chicken and road as\\nthe reader is trying to guess what happens next.\\nThis fact that words have rich linguistic relationships with other words that may\\nbe far away pervades language. Consider two more examples:\\n(9.4) The keys to the cabinet are on the table.\\n(9.5) I walked along the pond, and noticed one of the trees along the bank.\\n2 We say that in the ﬁrst example it corefers with the chicken, and in the second it corefers with the\\nroad; we’ll return to this in Chapter 23.'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13')]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break it down\n",
    "query = \"list the important points of the documents \"\n",
    "llm_response = qa_chain(query)\n",
    "# process_llm_response(llm_response)\n",
    "llm_response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAttention is a mechanism used in neural networks to selectively focus on certain parts of the input that are relevant for the task at hand. It can be used to weight the input tokens and collect information passed from lower layers to higher layers. An encoder is a component of a neural network that takes in the input data and converts it into a form that can be used by the rest of the network. In the context of self-attention and memory networks, an encoder is responsible for encoding the input tokens and passing them to the decoder or higher layers.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '\"Many American governments have passed laws to make the registration or voting process more difficult since 2009\" explain in three points?',\n",
       " 'result': \"\\n1. Voter ID laws: Many American governments have passed laws requiring voters to present a valid form of identification in order to vote. This has made the registration and voting process more difficult for those who may not have easy access to identification, such as low-income individuals or those without a driver's license.\\n\\n2. Restrictions on early voting and absentee ballots: Some states have implemented new laws limiting the availability of early voting and absentee ballots. This makes it harder for individuals who may have difficulty getting to the polls on Election Day to participate in the voting process.\\n\\n3. Purging of voter rolls: In some states, there has been a trend of purging voter rolls, which removes individuals from the list of registered voters. This can happen if a voter has not participated in recent elections or if there are errors in the registration process. This can make it difficult for individuals to vote, as they may not realize they have been purged and therefore are not registered to vote.\",\n",
       " 'source_documents': [Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2025-01-12T12:07:39-08:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='9.1 • A TTENTION 3\\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\\nlanguages, must agree in grammatical number with the verbare; in this case both are\\nplural. In English we can’t use a singular verb like is with a plural subject like keys\\n(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\\nto the side of a pond or river and not a ﬁnancial institution because of the context,\\nincluding words like pond. (We’ll discuss word senses more in Chapter 11.)\\nThe point of all these examples is that these contextual words that help us com-\\npute the meaning of words in context can be quite far away in the sentence or para-\\ngraph. Transformers can build contextual representations of word meaning, contex-\\ntual embeddings, by integrating the meaning of these helpful contextual words. In acontextual\\nembeddings\\ntransformer, layer by layer, we build up richer and richer contextualized representa-'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2025-01-12T12:07:39-08:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-12T12:07:39-08:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hemchandrann/Documents/vscode/Multimodal RAG/DATA/The Transformer.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': '/False'}, page_content='9.1 • A TTENTION 3\\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\\nlanguages, must agree in grammatical number with the verbare; in this case both are\\nplural. In English we can’t use a singular verb like is with a plural subject like keys\\n(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\\nto the side of a pond or river and not a ﬁnancial institution because of the context,\\nincluding words like pond. (We’ll discuss word senses more in Chapter 11.)\\nThe point of all these examples is that these contextual words that help us com-\\npute the meaning of words in context can be quite far away in the sentence or para-\\ngraph. Transformers can build contextual representations of word meaning, contex-\\ntual embeddings, by integrating the meaning of these helpful contextual words. In acontextual\\nembeddings\\ntransformer, layer by layer, we build up richer and richer contextualized representa-\\ntions of the meanings of input tokens. At each layer, we compute the representation\\nof a token i by combining information about i from the previous layer with infor-\\nmation about the neighboring tokens to produce a contextualized representation for\\neach word at each position.\\nAttention is the mechanism in the transformer that weighs and combines the\\nrepresentations from appropriate other tokens in the context from layerk−1 to build\\nthe representation for tokens in layer k.\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nLayer k+1\\nLayer k\\nself-attention distribution\\ncolumns corresponding to input tokens\\nFigure 9.2 The self-attention weight distribution α that is part of the computation of the\\nrepresentation for the word it at layer k +1. In computing the representation for it, we attend\\ndifferently to the various words at layer k, with darker shades indicating higher self-attention')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break it down\n",
    "query = \"\\\"Many American governments have passed laws to make the registration or voting process more difficult since 2009\\\" explain in three points?\"\n",
    "llm_response = qa_chain(query)\n",
    "# process_llm_response(llm_response)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
